#imports Stanford’s Dependency Parser and sets up environment.
from nltk.parse.stanford import StanfordDependencyParser as sparse
pathmodelsjar = '/Users/PATH TO MODELS JAR'
pathjar = '/Users/PATH TO STANFORD PARSER JAR'
depparse = sparse(path_to_jar=pathjar, path_to_models_jar=pathmodelsjar)

directory = '/Users/' + input('User: ') + '/Desktop/'
builderfile = directory + input('filename: ') + '.csv'

#Limits analysis to solely the dependency tree level of interest
def lmtc_pfc(ventral_stream, v2, litc_lmtc, litc_pfc, dobj):
	for tuple in ventral_stream:
		if tuple[0] == v2:
			litc_lmtc.append(tuple)
			if tuple[1] == 'dobj':
				dobj.append(tuple[2][0])
	for tuple in litc_lmtc:
		litc_pfc.append(tuple[1])



#Finds and arranges relevant oblique data
def oblique(ventral_stream, v3, met_case_marker, adv_nmod, search):
	for tuple in ventral_stream:
		if 'mod' in tuple[1] and search in tuple[2][0]:
				v3.append(tuple[2])
				for tuple in ventral_stream:
					if tuple[1] == 'case' and tuple[0] == v3[0]:
						met_case_marker.append(tuple[2])
					elif tuple[2] == v3[0] and 'adv' in tuple[1]:
						met_case_marker.append(tuple[0])



#Generates the array that will be passed on as an interpretable array.
def corpus_callosum(corpus_callosum1, met_case_marker, adv_nmod, dobj, v1, litc_pfc, litc_lmtc, v3):
	corpus_callosum1.append(v1[0][2][0])
	corpus_callosum1.append(dobj[0]) if len(dobj) != 0 else corpus_callosum1.append('0')
	corpus_callosum1.append(str(litc_pfc).replace(',', ' '))
	corpus_callosum1.append(litc_lmtc[0][0][0])
	if len(met_case_marker) != 0:
		corpus_callosum1.append(met_case_marker[0][0])
		corpus_callosum1.append(v3[0][0])
	elif len(met_case_marker) == 0 and len(adv_nmod) == 0:
		corpus_callosum1.append(int(0))
		corpus_callosum1.append(int(0))


#Prints and sends array to relevant next steps
def brocas(sentence, litc_pfc, litc_lmtc, array, TEST='non'):
	print('==============')
	if TEST == 'non':
		print(sentence)
		print('')
	if TEST == 'test':
		print('Grammatical Structure:')
		print(litc_pfc)
		print(' ')
		print('Lexical Items:')
		print(litc_lmtc)
		print(' ')
	#Builds the training data sheet for you, if selected.
	if TEST == 'build':
		Training_Data_Builder(array)
	print('Array: ')
	print(array)
	print('==============')



#Integrates all processes to create array and pass data on. Sanitizes lists from last search.
def occipital(sentence, TEST='non'):
	#Resets triggers and data failsafes
	v1 = []
	v2 = ''
	v3 = []
	met_case_marker = []
	dobj = []
	adv_nmod = []
	ventral_stream = []
	litc_lmtc = []
	litc_pfc = []
	corpus_callosum1 = []
	#components from Stanford's Dependency parser to create dep. tree.
	res = depparse.raw_parse(sentence)
	dep = res.__next__()
	ventral_stream = list(dep.triples())
	for tuple in ventral_stream:
		if tuple[1] == 'nsubj' or tuple[1] == 'nsubjpass':
			v1.append(tuple)
	if len(v1) > int(0):
		v2 = v1[0][0]
		lmtc_pfc(ventral_stream, v2, litc_lmtc, litc_pfc, dobj)
		oblique(litc_lmtc, ventral_stream, v3, met_case_marker, adv_nmod)
		corpus_callosum(corpus_callosum1, met_case_marker, adv_nmod, dobj, v1, litc_pfc, litc_lmtc, v3)
		brocas(sentence, litc_pfc, litc_lmtc, corpus_callosum1, TEST)
		#print(corpus_callosum1)
	elif len(v1) == 0:
		print('==============')
		print("No dependency structure to analyze")
		print(sentence)
		print('==============')

#Data packaging
import codecs
import csv

#takes data and saves it to a CSV to build training file.
def Training_Data_Builder(array):
	with codecs.open(builderfile, 'a', 'utf-8') as csvfile:
		databuilder = csv.writer(csvfile, delimiter=',',
				quotechar='|',
				quoting=csv.QUOTE_MINIMAL)
		databuilder.writerow(array)
	csvfile.close()

#Builds the training data from a printed corpus. Designate your corpus file, the search term(s) you’re looking for, and it’ll feed each one to occipital() in build mode. 
def pvc(file, search, function='non'):
	doc = codecs.open(file, 'r', 'utf-8')
	searchlines = doc.readlines()
	doc.close()
	for i, line in enumerate(searchlines):
		for item in search:
			if item in line:
				line2 = line.replace('\\', '').replace('}', '').replace('uc0u8232', '').replace('\'92', '\'').replace('a0', '').replace('\'93', '\"').replace('\'94', '\"').replace('\'96', ',').replace('\'97', ',').replace('f0fs24 ', '').replace('cf0 ', '')
				occipital(line2, function)

def test(sentence):
	res = depparse.raw_parse(sentence)
	dep = res.__next__()
	ventral_stream = list(dep.triples())
	for tuple in ventral_stream:
		print(tuple)

def cleanup(file):
	cleanfilename = directory + input('filename: ') + '.txt'
	doc = codecs.open(file, 'r', 'utf-8')
	searchlines = doc.readlines()
	doc.close()
	cdoc = codecs.open(cleanfilename, 'a', 'utf-8')
	for i, line in enumerate(searchlines):
		if '<file url=' not in line:
			cdoc.write(line)
	cdoc.close()

