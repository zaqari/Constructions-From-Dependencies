#imports Stanford’s Dependency Parser and sets up environment.
from nltk.parse.stanford import StanfordDependencyParser as sparse
pathmodelsjar = '/Users/ZaqRosen/nltk_data/stanford-english-corenlp-2016-01-10-models.jar'
pathjar = '/Users/ZaqRosen/nltk_data/stanford-parser/stanford-parser.jar'
depparse = sparse(path_to_jar=pathjar, path_to_models_jar=pathmodelsjar)

#Limits analysis to solely the dependency tree level of interest
def lmtc_pfc(ventral_stream, v2, litc_lmtc, litc_pfc, dobj):
	for tuple in ventral_stream:
		if tuple[0] == v2:
			litc_lmtc.append(tuple)
			if tuple[1] == 'dobj':
				dobj.append(tuple[2][0])
	for tuple in litc_lmtc:
		litc_pfc.append(tuple[1])

#Finds and arranges relevant oblique data
def oblique(litc_lmtc, ventral_stream, v3, met_case_marker, adv_nmod):
		v3.append(litc_lmtc[len(litc_lmtc)-1])
		if v3[0][1] == 'nmod':
			for tuple in ventral_stream:
				if tuple[0] == v3[0][2] and tuple[1] == 'case':
					met_case_marker.append(tuple[2])
		if v3[0][1] == 'advmod':
			for tuple in ventral_stream:
				met_case_marker.append(v3[0][2])
				if tuple[0] == v3[0][2] and 'nmod' in tuple[1]:
					adv_nmod.append(tuple[2])
				else:
					adv_nmod.append(int(0))

#Generates the array that will be passed to KellyGEN
def corpus_callosum(corpus_callosum1, met_case_marker, adv_nmod, dobj, v1, litc_pfc, litc_lmtc, v3):
	corpus_callosum1.append(v1[0][2][0])
	corpus_callosum1.append(dobj[0]) if len(dobj) != 0 else corpus_callosum1.append('0')
	corpus_callosum1.append(str(litc_pfc).replace(',', ' '))
	corpus_callosum1.append(litc_lmtc[0][0][0])
	if len(met_case_marker) != 0:
		if v3[0][1] == 'nmod':
			corpus_callosum1.append(met_case_marker[0][0])
			corpus_callosum1.append(v3[0][2][0])
		elif v3[0][1] == 'advmod':
			corpus_callosum1.append(met_case_marker[0][0])
			corpus_callosum1.append(adv_nmod[0])
	elif len(met_case_marker) == 0:
		corpus_callosum1.append(int(0))
		corpus_callosum1.append(int(0))

#Prints and sends array to relevant next steps
def brocas(sentence, litc_pfc, litc_lmtc, array, TEST='non'):
	print('==============')
	if TEST == 'non':
		print(sentence)
		print('')
	if TEST == 'test':
		print('Grammatical Structure:')
		print(litc_pfc)
		print(' ')
		print('Lexical Items:')
		print(litc_lmtc)
		print(' ')
	#Builds the training data sheet for you, if selected.
	if TEST == 'build':
		Training_Data_Builder(array)
	print('Array: ')
	print(array)
	print('==============')



#Integrates all processes to create array and pass data on. Sanitizes lists from last search.
def occipital(sentence, TEST='non'):
	#Resets triggers and data failsafes
	v1 = []
	v2 = ''
	v3 = []
	met_case_marker = []
	dobj = []
	adv_nmod = []
	ventral_stream = []
	litc_lmtc = []
	litc_pfc = []
	corpus_callosum1 = []
	#components from Stanford's Dependency parser to create dep. tree.
	res = depparse.raw_parse(sentence)
	dep = res.__next__()
	ventral_stream = list(dep.triples())
	for tuple in ventral_stream:
		if tuple[1] == 'nsubj' or tuple[1] == 'nsubjpass':
			v1.append(tuple)
	if len(v1) > int(0):
		v2 = v1[0][0]
		lmtc_pfc(ventral_stream, v2, litc_lmtc, litc_pfc, dobj)
		oblique(litc_lmtc, ventral_stream, v3, met_case_marker, adv_nmod)
		corpus_callosum(corpus_callosum1, met_case_marker, adv_nmod, dobj, v1, litc_pfc, litc_lmtc, v3)
		brocas(sentence, litc_pfc, litc_lmtc, corpus_callosum1, TEST)
		#print(corpus_callosum1)
	elif len(v1) == 0:
		print('==============')
		print("No dependency structure to analyze")
		print(sentence)
		print('==============')

#Data packaging
import codecs
import csv

#takes data and saves it to a CSV to build training file.
def Training_Data_Builder(array):
	with codecs.open('/Users/ZaqRosen/Desktop/Met_Training_Data.csv', 'a', 'utf-8') as csvfile:
		databuilder = csv.writer(csvfile, delimiter=',',
				quotechar='|',
				quoting=csv.QUOTE_MINIMAL)
		databuilder.writerow(array)
	csvfile.close()

#Builds the training data from a printed corpus. Designate your corpus file, the search term(s) you’re looking for, and it’ll feed each one to occipital() in build mode. 
def pvc(file, search, function='non'):
	doc = codecs.open(file, 'r', 'utf-8')
	searchlines = doc.readlines()
	doc.close()
	for i, line in enumerate(searchlines):
		if search in line:
			line2 = line.replace('\\', '').replace('}', '').replace('uc0u8232', '').replace('\'92', '\'').replace('a0', '').replace('\'93', '\"').replace('\'94', '\"').replace('\'96', ',').replace('\'97', ',').replace('f0fs24 ', '').replace('cf0 ', '')
			occipital(line2, function)

def test(sentence):
	res = depparse.raw_parse(sentence)
	dep = res.__next__()
	ventral_stream = list(dep.triples())
	for tuple in ventral_stream:
		print(tuple)

#Builds usable corpus data from search term, excluding titles.
def corpus_builder(file, search):
	doc = codecs.open(file, 'r', 'utf-8')
	searchlines = doc.readlines()
	doc.close()
	doc2 = codecs.open('/Users/ZaqRosen/Desktop/Corpus.rtf', 'a', 'utf-8')
	for i, line in enumerate(searchlines):
		if search in line:
			line2 = line.replace('\\', '').replace('}', '').replace('uc0u8232', '').replace('\'92', '\'').replace('a0', '').replace('\'93', '\"').replace('\'94', '\"').replace('\'96', ',').replace('\'97', ',').replace('f0fs24 ', '').replace('cf0 ', '')
			res = depparse.raw_parse(line2)
			dep = res.__next__()
			streamofconsciousness = list(dep.triples())
			if all(tuple[1] != 'compound' for tuple in streamofconsciousness):
				doc2.write(line2)
	doc2.close()
